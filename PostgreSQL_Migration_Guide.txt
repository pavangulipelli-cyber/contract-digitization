================================================================================
SQLite3 to PostgreSQL Migration Guide
Contract Digitization Backend
================================================================================

OVERVIEW
--------
Migrate the contract digitization backend from SQLite3 to PostgreSQL for 
improved scalability, concurrent access, and production readiness.

================================================================================
PREREQUISITES
================================================================================

1. Install PostgreSQL
---------------------
- Windows: Download from https://www.postgresql.org/download/windows/
- Linux/Mac: sudo apt-get install postgresql OR brew install postgresql
- Default port: 5432
- During installation, set a password for the postgres user

2. Install Database Tools
--------------------------
# Python (for FastAPI)
pip install psycopg2-binary

# OR
pip install psycopg2

# Node.js (for Express)
npm install pg

================================================================================
STEP-BY-STEP MIGRATION
================================================================================

STEP 1: Create PostgreSQL Database
-----------------------------------

# Login to PostgreSQL
psql -U postgres

# Create database
CREATE DATABASE contract_ai_db;

# Create user (optional, for security)
CREATE USER contract_app WITH PASSWORD 'your_secure_password';
GRANT ALL PRIVILEGES ON DATABASE contract_ai_db TO contract_app;

# Connect to the new database
\c contract_ai_db

# Exit
\q

================================================================================
STEP 2: Convert SQLite Schema to PostgreSQL
================================================================================

Important Differences:
----------------------
- SQLite uses INTEGER PRIMARY KEY â†’ PostgreSQL uses SERIAL or BIGSERIAL
- SQLite AUTOINCREMENT â†’ PostgreSQL SERIAL
- SQLite boolean (0/1) â†’ PostgreSQL BOOLEAN (true/false)
- SQLite datetime() â†’ PostgreSQL NOW() or CURRENT_TIMESTAMP
- SQLite has no schema enforcement â†’ PostgreSQL strictly enforces data types

Create PostgreSQL Schema:
-------------------------
File: fastapi_server/data/contract_ai_schema_postgres.sql

-- Documents Table
CREATE TABLE IF NOT EXISTS documents (
    id VARCHAR(100) PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    document_type VARCHAR(100),
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    uploaded_by VARCHAR(200),
    status VARCHAR(50) DEFAULT 'Pending Review',
    current_version_id VARCHAR(100),
    current_version_number INTEGER DEFAULT 1,
    storage_ref VARCHAR(500),
    total_pages INTEGER,
    file_size_kb INTEGER,
    tags TEXT,
    notes TEXT
);

-- Document Versions Table
CREATE TABLE IF NOT EXISTS document_versions (
    id VARCHAR(100) PRIMARY KEY,
    document_id VARCHAR(100) NOT NULL,
    version_number INTEGER NOT NULL,
    is_latest BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR(200),
    status VARCHAR(50) DEFAULT 'Pending Review',
    storage_ref VARCHAR(500),
    notes TEXT,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
    UNIQUE(document_id, version_number)
);

-- Attributes Table
CREATE TABLE IF NOT EXISTS attributes (
    id VARCHAR(200) PRIMARY KEY,
    document_id VARCHAR(100) NOT NULL,
    version_id VARCHAR(100) NOT NULL,
    attribute_key VARCHAR(100),
    name VARCHAR(200) NOT NULL,
    category VARCHAR(100),
    section VARCHAR(200),
    page INTEGER,
    confidence_score DECIMAL(5,2),
    confidence_level VARCHAR(50),
    extracted_value TEXT,
    corrected_value TEXT,
    highlighted_text TEXT,
    status VARCHAR(50) DEFAULT 'Pending',
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
    FOREIGN KEY (version_id) REFERENCES document_versions(id) ON DELETE CASCADE
);

-- Attribute Reviews Table
CREATE TABLE IF NOT EXISTS attribute_reviews (
    id SERIAL PRIMARY KEY,
    attribute_id VARCHAR(200) NOT NULL,
    document_id VARCHAR(100) NOT NULL,
    version_id VARCHAR(100) NOT NULL,
    reviewed_by VARCHAR(200),
    reviewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    old_value TEXT,
    new_value TEXT,
    comment TEXT,
    FOREIGN KEY (attribute_id) REFERENCES attributes(id) ON DELETE CASCADE,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
    FOREIGN KEY (version_id) REFERENCES document_versions(id) ON DELETE CASCADE
);

-- Indexes for Performance
CREATE INDEX idx_doc_versions_doc_id ON document_versions(document_id);
CREATE INDEX idx_doc_versions_latest ON document_versions(document_id, is_latest);
CREATE INDEX idx_attributes_doc_id ON attributes(document_id);
CREATE INDEX idx_attributes_version_id ON attributes(version_id);
CREATE INDEX idx_attr_reviews_attr_id ON attribute_reviews(attribute_id);

================================================================================
STEP 3: Export Data from SQLite
================================================================================

# Navigate to database directory
cd fastapi_server/data

# Option A: Export to SQL format
sqlite3 contract_ai_versioned.db .dump > sqlite_dump.sql

# Option B: Export to CSV (Recommended for safer migration)
sqlite3 contract_ai_versioned.db <<EOF
.headers on
.mode csv
.output documents.csv
SELECT * FROM documents;
.output document_versions.csv
SELECT * FROM document_versions;
.output attributes.csv
SELECT * FROM attributes;
.output attribute_reviews.csv
SELECT * FROM attribute_reviews;
.quit
EOF

================================================================================
STEP 4: Import Data to PostgreSQL
================================================================================

Option A: Using CSV files (Recommended)
----------------------------------------
psql -U postgres -d contract_ai_db

# Inside psql:
\copy documents FROM 'documents.csv' DELIMITER ',' CSV HEADER;
\copy document_versions FROM 'document_versions.csv' DELIMITER ',' CSV HEADER;
\copy attributes FROM 'attributes.csv' DELIMITER ',' CSV HEADER;
\copy attribute_reviews FROM 'attribute_reviews.csv' DELIMITER ',' CSV HEADER;

Option B: Using pgloader (automatic conversion)
------------------------------------------------
# Install pgloader
# Ubuntu: sudo apt-get install pgloader
# Mac: brew install pgloader

# Run migration
pgloader sqlite://contract_ai_versioned.db postgresql://postgres:password@localhost/contract_ai_db

================================================================================
STEP 5: Update FastAPI Backend Code
================================================================================

File: fastapi_server/main.py

1. Replace SQLite imports:
--------------------------
# OLD:
import sqlite3

# NEW:
import psycopg2
from psycopg2.extras import RealDictCursor

2. Update database connection:
-------------------------------
# OLD SQLite connection:
def get_db():
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA foreign_keys = ON")
    try:
        yield conn
    finally:
        conn.close()

# NEW PostgreSQL connection:
import os

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432"),
    "database": os.getenv("DB_NAME", "contract_ai_db"),
    "user": os.getenv("DB_USER", "postgres"),
    "password": os.getenv("DB_PASSWORD", "your_password")
}

def get_db():
    conn = psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)
    try:
        yield conn
    finally:
        conn.close()

3. Update query syntax:
------------------------
# SQLite uses ? placeholders
# PostgreSQL uses %s placeholders

# OLD:
cursor.execute("SELECT * FROM documents WHERE id = ?", (doc_id,))

# NEW:
cursor.execute("SELECT * FROM documents WHERE id = %s", (doc_id,))

# Boolean handling
# OLD: isLatest = 1 or 0
# NEW: is_latest = TRUE or FALSE

# Column names
# SQLite: case-insensitive (isLatest, IsLatest, islatest all work)
# PostgreSQL: case-sensitive, use snake_case (is_latest)

4. Update auto-seeding function:
---------------------------------
def ensure_seeded():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # Check if tables exist
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        
        existing_tables = {row[0] for row in cursor.fetchall()}
        required_tables = {"documents", "document_versions", "attributes", "attribute_reviews"}
        
        if not required_tables.issubset(existing_tables):
            print("ðŸ“¦ Creating schema and seeding database...")
            
            # Read and execute schema file
            with open("./data/contract_ai_schema_postgres.sql", "r") as f:
                cursor.execute(f.read())
            
            conn.commit()
            print("âœ… Database seeded successfully")
        
        cursor.close()
        conn.close()
    except Exception as e:
        print(f"âŒ Error: {e}")

================================================================================
STEP 6: Update Express Backend Code
================================================================================

File: express_server/index.js

1. Replace sqlite3:
-------------------
// OLD:
const sqlite3 = require('sqlite3').verbose();

// NEW:
const { Pool } = require('pg');

2. Database connection:
-----------------------
const pool = new Pool({
  host: process.env.DB_HOST || 'localhost',
  port: process.env.DB_PORT || 5432,
  database: process.env.DB_NAME || 'contract_ai_db',
  user: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD || 'your_password',
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

3. Update queries:
------------------
// OLD:
db.get("SELECT * FROM documents WHERE id = ?", [docId], (err, row) => {
  // ...
});

// NEW:
const result = await pool.query("SELECT * FROM documents WHERE id = $1", [docId]);
const row = result.rows[0];

4. Convert callbacks to async/await:
-------------------------------------
app.get('/api/documents/:id', async (req, res) => {
  try {
    const { id } = req.params;
    const result = await pool.query("SELECT * FROM documents WHERE id = $1", [id]);
    
    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'Document not found' });
    }
    
    res.json(result.rows[0]);
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Database error' });
  }
});

================================================================================
STEP 7: Update Environment Variables
================================================================================

Create .env file in project root:
----------------------------------
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=contract_ai_db
DB_USER=postgres
DB_PASSWORD=your_secure_password

# API Configuration
VITE_API_BASE_URL=http://localhost:8000
PORT=8000

Update requirements.txt:
-------------------------
fastapi==0.115.0
uvicorn[standard]==0.32.0
pydantic==2.10.0
python-multipart==0.0.12
psycopg2-binary==2.9.9

Update package.json (Express):
-------------------------------
npm install pg --save

================================================================================
STEP 8: Testing
================================================================================

1. Test PostgreSQL connection:
-------------------------------
psql -U postgres -d contract_ai_db -c "SELECT COUNT(*) FROM documents;"

2. Start FastAPI server:
-------------------------
cd fastapi_server
pip install -r requirements.txt
uvicorn main:app --reload --port 8000

3. Test API endpoints:
----------------------
curl http://localhost:8000/health
curl http://localhost:8000/api/documents
curl http://localhost:8000/api/documents/doc-001/attributes

4. Test frontend:
-----------------
cd ..
npm run dev
# Navigate to http://localhost:8080 and test all features

================================================================================
STEP 9: Data Validation
================================================================================

Connect to PostgreSQL:
----------------------
psql -U postgres -d contract_ai_db

Verify record counts:
---------------------
SELECT 'documents' AS table_name, COUNT(*) FROM documents
UNION ALL
SELECT 'document_versions', COUNT(*) FROM document_versions
UNION ALL
SELECT 'attributes', COUNT(*) FROM attributes
UNION ALL
SELECT 'attribute_reviews', COUNT(*) FROM attribute_reviews;

Check latest versions:
----------------------
SELECT document_id, version_number, is_latest 
FROM document_versions 
WHERE is_latest = TRUE;

Verify foreign keys:
--------------------
SELECT COUNT(*) FROM attributes 
WHERE version_id NOT IN (SELECT id FROM document_versions);

================================================================================
COMMON ISSUES & SOLUTIONS
================================================================================

Issue 1: Boolean Conversion
----------------------------
# SQLite stores as 0/1, PostgreSQL needs TRUE/FALSE
# Solution: Use CASE in migration
UPDATE document_versions 
SET is_latest = CASE WHEN is_latest = '1' THEN TRUE ELSE FALSE END;

Issue 2: Column Name Case Sensitivity
--------------------------------------
# PostgreSQL converts to lowercase unless quoted
# Recommendation: Use snake_case consistently
isLatest â†’ is_latest
versionNumber â†’ version_number

Issue 3: Date/Time Formats
---------------------------
# SQLite: datetime('now')
# PostgreSQL: CURRENT_TIMESTAMP or NOW()

Issue 4: Connection Pooling
----------------------------
# For production, use connection pooling
from psycopg2 import pool

db_pool = psycopg2.pool.SimpleConnectionPool(
    minconn=1,
    maxconn=20,
    **DB_CONFIG
)

================================================================================
ROLLBACK PLAN
================================================================================

If migration fails, keep SQLite backup:

# Backup before migration
cp fastapi_server/data/contract_ai_versioned.db \
   fastapi_server/data/contract_ai_versioned.db.backup

# Restore if needed
cp fastapi_server/data/contract_ai_versioned.db.backup \
   fastapi_server/data/contract_ai_versioned.db

================================================================================
PERFORMANCE OPTIMIZATION (POST-MIGRATION)
================================================================================

Add indexes:
------------
CREATE INDEX idx_attributes_category ON attributes(category);
CREATE INDEX idx_attributes_confidence ON attributes(confidence_score DESC);
CREATE INDEX idx_doc_status ON documents(status);

Analyze tables:
---------------
ANALYZE documents;
ANALYZE document_versions;
ANALYZE attributes;
ANALYZE attribute_reviews;

Enable query logging (development only):
-----------------------------------------
ALTER DATABASE contract_ai_db SET log_statement = 'all';

================================================================================
ESTIMATED TIME
================================================================================
- Setup PostgreSQL: 30 minutes
- Schema migration: 1 hour
- Code updates: 2-3 hours
- Testing: 1-2 hours
- Total: 5-6 hours

================================================================================
SUCCESS CHECKLIST
================================================================================
[ ] PostgreSQL installed and running
[ ] Database and user created
[ ] Schema created successfully
[ ] All data imported (verify counts)
[ ] FastAPI code updated and tested
[ ] Express code updated and tested
[ ] Environment variables configured
[ ] All API endpoints working
[ ] Frontend functionality verified
[ ] Performance acceptable
[ ] Backup of SQLite database kept

================================================================================
END OF MIGRATION GUIDE
================================================================================
